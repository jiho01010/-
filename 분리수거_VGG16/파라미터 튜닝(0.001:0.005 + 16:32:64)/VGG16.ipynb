{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG16 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 다운로드\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 준비 함수: train+val 병합, test 로더 생성\n",
    "def prepare_datasets(base_dir,\n",
    "                     train_txt=\"one-indexed-files-notrash_train.txt\",\n",
    "                     val_txt=\"one-indexed-files-notrash_val.txt\",\n",
    "                     test_txt=\"one-indexed-files-notrash_test.txt\",\n",
    "                     img_subdir=\"Garbage classification\",\n",
    "                     img_size=(224,224),\n",
    "                     batch_size=16):\n",
    "    img_dir = os.path.join(base_dir, img_subdir)\n",
    "    # transform\n",
    "    transform = transforms.Compose([transforms.Resize(img_size), transforms.ToTensor()])\n",
    "    full_ds = datasets.ImageFolder(img_dir, transform=transform)\n",
    "    class_names = full_ds.classes\n",
    "\n",
    "    # 파일명→인덱스 맵\n",
    "    name_to_idx = {os.path.basename(p): idx for idx,(p,_) in enumerate(full_ds.samples)}\n",
    "    def _load_idxs(txt):\n",
    "        path = os.path.join(base_dir, txt)\n",
    "        idxs = []\n",
    "        with open(path) as f:\n",
    "            for line in f:\n",
    "                fname = line.strip().split()[0]\n",
    "                idxs.append(name_to_idx[fname])\n",
    "        return idxs\n",
    "\n",
    "    train_idxs = _load_idxs(train_txt)\n",
    "    val_idxs   = _load_idxs(val_txt)\n",
    "    test_idxs  = _load_idxs(test_txt)\n",
    "\n",
    "    # train+val 합치기\n",
    "    train_val_idxs = train_idxs + val_idxs\n",
    "    train_val_ds = Subset(full_ds, train_val_idxs)\n",
    "    test_ds      = Subset(full_ds, test_idxs)\n",
    "\n",
    "    train_val_loader = DataLoader(train_val_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader      = DataLoader(test_ds,      batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    print(\"클래스:\", class_names)\n",
    "    print(\"Train+Val 샘플 수:\", len(train_val_ds))\n",
    "    print(\"Test 샘플 수:\",  len(test_loader.dataset))\n",
    "\n",
    "    return train_val_ds, train_val_loader, test_loader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold 교차검증 및 최종 테스트 평가를 수행하는 함수\n",
    "# tuning_index: 튜닝 식별용 인덱스(보고서 파일명에 사용)\n",
    "# train_val_ds: train과 validation이 합쳐진 Subset\n",
    "# test_loader: 최종 테스트 데이터 로더\n",
    "# class_names: 클래스 이름 리스트\n",
    "# k: 교차검증 Fold 수\n",
    "# lr: 학습률\n",
    "# batch_size: 배치 크기\n",
    "# num_epochs: epoch 수\n",
    "# freeze_epochs: feature extractor 동결 유지 epoch 수\n",
    "def kfold_train_and_evaluate(train_val_ds,\n",
    "                             test_loader,\n",
    "                             class_names,\n",
    "                             tuning_index,\n",
    "                             lr,\n",
    "                             batch_size,\n",
    "                             num_epochs,\n",
    "                             k=5,\n",
    "                             freeze_epochs=0,\n",
    "                             device=None):\n",
    "    # 1) 재현성 확보\n",
    "    torch.manual_seed(42); np.random.seed(42)\n",
    "\n",
    "    # 2) 디바이스 설정\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 3) 원본 전체 Dataset과 train+val 인덱스 분리\n",
    "    full_dataset = train_val_ds.dataset\n",
    "    all_idxs     = train_val_ds.indices\n",
    "    all_labels   = [full_dataset.samples[i][1] for i in all_idxs]\n",
    "\n",
    "    # 4) StratifiedKFold 설정\n",
    "    skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # 기록용 리스트\n",
    "    cv_epoch_history = []\n",
    "    test_fold_history = []\n",
    "\n",
    "    # --- K-Fold Cross-Validation ---\n",
    "    for fold, (tr_idx, val_idx) in enumerate(\n",
    "            skf.split(np.zeros(len(all_labels)), all_labels), start=1):\n",
    "        # Subset & DataLoader 준비\n",
    "        tr_sub = Subset(full_dataset, [all_idxs[i] for i in tr_idx])\n",
    "        va_sub = Subset(full_dataset, [all_idxs[i] for i in val_idx])\n",
    "        tr_loader = DataLoader(tr_sub, batch_size=batch_size, shuffle=True)\n",
    "        va_loader = DataLoader(va_sub, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # 모델 초기화\n",
    "        model = models.vgg16(pretrained=True)\n",
    "        model.classifier[-1] = torch.nn.Linear(4096, len(class_names))\n",
    "        model.to(device)\n",
    "        # feature extractor freeze\n",
    "        for p in model.features.parameters(): p.requires_grad = False\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "            optimizer, step_size=max(1, num_epochs//2), gamma=0.1)\n",
    "\n",
    "        # Epoch별 학습 + 검증\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            # --- Train ---\n",
    "            model.train()\n",
    "            if epoch == freeze_epochs + 1:\n",
    "                # freeze 해제\n",
    "                for p in model.features.parameters(): p.requires_grad = True\n",
    "                optimizer.add_param_group({'params': model.features.parameters()})\n",
    "\n",
    "            running_loss = 0.0\n",
    "            for imgs, lbls in tr_loader:\n",
    "                imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(model(imgs), lbls)\n",
    "                loss.backward(); optimizer.step()\n",
    "                running_loss += loss.item() * imgs.size(0)\n",
    "            scheduler.step()\n",
    "\n",
    "            # --- Validation ---\n",
    "            model.eval()\n",
    "            preds, targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for imgs, lbls in va_loader:\n",
    "                    imgs = imgs.to(device)\n",
    "                    out = model(imgs).argmax(dim=1).cpu().numpy()\n",
    "                    preds.extend(out)\n",
    "                    targets.extend(lbls.numpy())\n",
    "\n",
    "            # 메트릭 계산\n",
    "            report = classification_report(targets, preds, output_dict=True, zero_division=0)\n",
    "            macro  = report[\"macro avg\"]\n",
    "            acc    = np.mean(np.array(preds) == np.array(targets))\n",
    "\n",
    "            # 기록 저장\n",
    "            cv_epoch_history.append({\n",
    "                \"tuning_index\": tuning_index,\n",
    "                \"fold\":         fold,\n",
    "                \"epoch\":        epoch,\n",
    "                \"val_acc\":      acc,\n",
    "                \"val_prec\":     macro[\"precision\"],\n",
    "                \"val_recall\":   macro[\"recall\"],\n",
    "                \"val_f1\":       macro[\"f1-score\"]\n",
    "            })\n",
    "\n",
    "            print(f\"[Fold {fold}] Epoch {epoch}/{num_epochs}  \"\n",
    "                  f\"Loss: {running_loss/len(tr_sub):.4f}  \"\n",
    "                  f\"Val Acc: {acc:.4f}  \"\n",
    "                  f\"Val F1: {macro['f1-score']:.4f}\")\n",
    "        \n",
    "        # --- Test Evaluation after this fold ---\n",
    "        model.eval()\n",
    "        test_preds, test_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for imgs, lbls in test_loader:\n",
    "                imgs = imgs.to(device)\n",
    "                out = model(imgs).argmax(dim=1).cpu().numpy()\n",
    "                test_preds.extend(out)\n",
    "                test_targets.extend(lbls.numpy())\n",
    "\n",
    "        test_report = classification_report(test_targets, test_preds, output_dict=True, zero_division=0)\n",
    "        test_macro  = test_report[\"macro avg\"]\n",
    "        test_acc    = np.mean(np.array(test_preds) == np.array(test_targets))\n",
    "\n",
    "        test_fold_history.append({\n",
    "            \"tuning_index\": tuning_index,\n",
    "            \"fold\":         fold,\n",
    "            \"test_acc\":     test_acc,\n",
    "            \"test_prec\":    test_macro[\"precision\"],\n",
    "            \"test_recall\":  test_macro[\"recall\"],\n",
    "            \"test_f1\":      test_macro[\"f1-score\"]\n",
    "        })\n",
    "\n",
    "        print(f\"▶ [Fold {fold}] Test Acc: {test_acc:.4f}  \"\n",
    "              f\"Test F1: {test_macro['f1-score']:.4f}\")\n",
    "\n",
    "        # 메모리 정리\n",
    "        del model, optimizer, scheduler\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # 결과 디렉토리 생성\n",
    "    os.makedirs(\"tuning_reports\", exist_ok=True)\n",
    "\n",
    "    # CV metrics 저장\n",
    "    df_cv = pd.DataFrame(cv_epoch_history)\n",
    "    cv_path = f\"tuning_reports/tuning_{tuning_index}_lr{lr}_bs{batch_size}_cv.csv\"\n",
    "    df_cv.to_csv(cv_path, index=False)\n",
    "    print(f\"✅ CV metrics saved to: {cv_path}\")\n",
    "\n",
    "    # Test metrics 저장\n",
    "    df_test = pd.DataFrame(test_fold_history)\n",
    "    test_path = f\"tuning_reports/tuning_{tuning_index}_lr{lr}_bs{batch_size}_test.csv\"\n",
    "    df_test.to_csv(test_path, index=False)\n",
    "    print(f\"✅ Test metrics saved to: {test_path}\")\n",
    "\n",
    "    # --- 전체 Train+Val 데이터로 최종 모델 재학습 및 저장 ---\n",
    "    full_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = models.vgg16(pretrained=True)\n",
    "    model.classifier[-1] = torch.nn.Linear(4096, len(class_names))\n",
    "    model.to(device)\n",
    "    for p in model.features.parameters(): p.requires_grad = False\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=max(1, num_epochs//2), gamma=0.1)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        if epoch == freeze_epochs + 1:\n",
    "            for p in model.features.parameters(): p.requires_grad = True\n",
    "            optimizer.add_param_group({'params': model.features.parameters()})\n",
    "        for imgs, lbls in full_loader:\n",
    "            imgs, lbls = imgs.to(device), lbls.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(imgs), lbls)\n",
    "            loss.backward(); optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    final_path = f\"tuning_reports/tuning_{tuning_index}_lr{lr}_bs{batch_size}_final_model.pth\"\n",
    "    torch.save(model.state_dict(), final_path)\n",
    "    print(f\"✅ Final model saved to: {final_path}\")\n",
    "\n",
    "\n",
    "    return df_cv, df_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 코드실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 준비\n",
    "base_dir = \"./archive/Garbage classification\"\n",
    "train_val_ds, train_val_loader, test_loader, classes = prepare_datasets(\n",
    "    base_dir,\n",
    "    test_txt=\"one-indexed-files-notrash_test.txt\",\n",
    "    batch_size=16\n",
    ")\n",
    "# 2-1) K-Fold CV & Test 평가\n",
    "cv_df, test_metrics, epoch_history = kfold_train_and_evaluate(\n",
    "    train_val_ds, test_loader, classes,\n",
    "    tuning_index=1, lr=0.001, batch_size=16, num_epochs=10, freeze_epochs=3\n",
    ")\n",
    "print(\"=== CV Results ===\\n\", cv_df)\n",
    "print(\"=== Test Metrics ===\\n\", test_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 준비\n",
    "base_dir = \"./archive/Garbage classification\"\n",
    "train_val_ds, train_val_loader, test_loader, classes = prepare_datasets(\n",
    "    base_dir,\n",
    "    test_txt=\"one-indexed-files-notrash_test.txt\",\n",
    "    batch_size=32\n",
    ")\n",
    "# 2-2) K-Fold CV & Test 평가\n",
    "df_cv_2, df_test_2 = kfold_train_and_evaluate(\n",
    "    train_val_ds, test_loader, classes,\n",
    "    tuning_index=2, lr=0.001, batch_size=32, num_epochs=10, freeze_epochs=3\n",
    ")\n",
    "print(\"=== CV Results ===\\n\", df_cv_2)\n",
    "print(\"=== Test Metrics ===\\n\", df_test_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 준비\n",
    "base_dir = \"./archive/Garbage classification\"\n",
    "train_val_ds, train_val_loader, test_loader, classes = prepare_datasets(\n",
    "    base_dir,\n",
    "    test_txt=\"one-indexed-files-notrash_test.txt\",\n",
    "    batch_size=64\n",
    ")\n",
    "# 2-3) K-Fold CV & Test 평가\n",
    "df_cv_3, df_test_3 = kfold_train_and_evaluate(\n",
    "    train_val_ds, test_loader, classes,\n",
    "    tuning_index=3, lr=0.001, batch_size=64, num_epochs=10, freeze_epochs=3\n",
    ")\n",
    "print(\"=== CV Results ===\\n\", df_cv_3)\n",
    "print(\"=== Test Metrics ===\\n\", df_test_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-4) K-Fold CV & Test 평가\n",
    "df_cv_4, df_test_4 = kfold_train_and_evaluate(\n",
    "    train_val_ds, test_loader, classes,\n",
    "    tuning_index=4, lr=0.005, batch_size=16, num_epochs=10\n",
    ")\n",
    "print(\"=== CV Results ===\\n\", df_cv_4)\n",
    "print(\"=== Test Metrics ===\\n\", df_test_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 준비\n",
    "base_dir = \"./archive/Garbage classification\"\n",
    "train_val_ds, train_val_loader, test_loader, classes = prepare_datasets(\n",
    "    base_dir,\n",
    "    test_txt=\"one-indexed-files-notrash_test.txt\",\n",
    "    batch_size=32\n",
    ")\n",
    "# 2-5) K-Fold CV & Test 평가\n",
    "df_cv_5, df_test_5 = kfold_train_and_evaluate(\n",
    "    train_val_ds, test_loader, classes,\n",
    "    tuning_index=5, lr=0.005, batch_size=32, num_epochs=10\n",
    ")\n",
    "print(\"=== CV Results ===\\n\", df_cv_5)\n",
    "print(\"=== Test Metrics ===\\n\", df_test_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 데이터 준비\n",
    "base_dir = \"./archive/Garbage classification\"\n",
    "train_val_ds, train_val_loader, test_loader, classes = prepare_datasets(\n",
    "    base_dir,\n",
    "    test_txt=\"one-indexed-files-notrash_test.txt\",\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# 2-6) K-Fold CV & Test 평가\n",
    "df_cv_6, df_test_6 = kfold_train_and_evaluate(\n",
    "    train_val_ds, test_loader, classes,\n",
    "    tuning_index=6, lr=0.005, batch_size=64, num_epochs=10\n",
    ")\n",
    "print(\"=== CV Results ===\\n\", df_cv_6)\n",
    "print(\"=== Test Metrics ===\\n\", df_test_6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
